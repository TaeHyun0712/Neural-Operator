{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"10qnOKkwX-gqzQSQdc371TC80_Av3Ehnw","timestamp":1691298642760},{"file_id":"1JLZoltsawPw-x1iwkI5NgRhySR8UkBKH","timestamp":1690541190190}],"gpuType":"T4","collapsed_sections":["BGyB42PFAdij","TsKT-PYPiQDu","3CIqLT2iiaAN","1-060cvzekQo","fgezvJB1Drtl","8Oov7jbze-V0","tErtt3qvfTH2"],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# Solve Partial Differential Equation with Physics-Informed DeepONet(PI-DeepONet)\n","\n","본 예제에서는 Physic-Informed DeepONet(PI-DeepONet)를 통해 Burger's Equation의 해를 계산하도록 훈련시키는 방법을 보여줍니다. 해당 코드는 \"Learning the solution operator of parametric partial differential equations with physics-informed DeepONets\"의 Section \"Burgers transport dynamics\"의 예제 코드를 바탕으로 구성되었습니다. 추후 다른 예제 코드가 궁금하실 경우 본문 하단에 논문과 예제 코드의 참조 링크 및 논문에서 제공하는 보충 데이터가 있으니, 참조 부탁 드립니다. 해당 논문에서 사용된 Burger's Equation의 형태는 아래와 같습니다.\n","\n","\\\\\n","\n","$$\\frac{ds}{dt} + s\\frac{ds}{dx} - ν\\frac{d^2s}{dx^2},\\quad (x,t) ∈ (0, 1) × (0, 1]$$\n","$$s(x, 0) = u(x), \\quad x ∈ (0, 1)$$\n","\n","\\\\\n","$t∈(0,1)$ 이며, 초기 조건 $u(x)$는 $GRF \\sim 𝒩(0,25^2(−Δ + 5^2I)^{−4})$ 내에서 생성됩니다. 방정식의 점도 계수는 0.01의 값을 사용합니다. 경계조건은 주기 경계 조건(Periodic Boundary Condition)으로 설정합니다. 상세한 경계조건은 아래와 같습니다.\n","\n","\\\\\n","$$s(0,t) = s(1,t),$$\n","$$\\frac{ds}{dt}(0,t)=\\frac{ds}{dt}(1,t)$$\n","\n","\\\\\n","본 예제 코드는 PI-DeepONet을 사용하여 초기 조건 $u(x)$를 1D Burger's Equation의 전체 시공간 솔루션 $s(x,t)$에 매핑하는 솔루션 연산자를 학습합니다."],"metadata":{"id":"yneQJ8HXGEIW"}},{"cell_type":"markdown","source":["## Contact Google Drive"],"metadata":{"id":"BGyB42PFAdij"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"TmbJPl2TAdij"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["cd \"/content/drive/My Drive/Burger\""],"metadata":{"id":"fDpn3qIiAlv-"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mRR_BT5vUnSw"},"source":["import numpy as onp\n","import scipy.io\n","from scipy.interpolate import griddata\n","import jax.numpy as np\n","from jax import random, grad, vmap, jit\n","from jax.example_libraries import optimizers\n","from jax.config import config\n","from jax.flatten_util import ravel_pytree\n","from jax.nn import relu, elu\n","import itertools\n","from functools import partial\n","from torch.utils import data\n","from tqdm import trange\n","import matplotlib.pyplot as plt\n","\n","%matplotlib inline"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Define Model"],"metadata":{"id":"uz_c1T5t_vjC"}},{"cell_type":"markdown","source":["### Multi-Layer Perceptron(MLP)\n"],"metadata":{"id":"TsKT-PYPiQDu"}},{"cell_type":"code","metadata":{"id":"rTZSJ2taqCPZ"},"source":["# Define MLP\n","def MLP(layers, activation=relu):\n","  ''' Vanilla MLP'''\n","  def init(rng_key):\n","      def init_layer(key, d_in, d_out):\n","          k1, k2 = random.split(key)\n","          glorot_stddev = 1. / np.sqrt((d_in + d_out) / 2.)\n","          W = glorot_stddev * random.normal(k1, (d_in, d_out))\n","          b = np.zeros(d_out)\n","          return W, b\n","      key, *keys = random.split(rng_key, len(layers))\n","      params = list(map(init_layer, keys, layers[:-1], layers[1:]))\n","      return params\n","  def apply(params, inputs):\n","      for W, b in params[:-1]:\n","          outputs = np.dot(inputs, W) + b\n","          inputs = activation(outputs)\n","      W, b = params[-1]\n","      outputs = np.dot(inputs, W) + b\n","      return outputs\n","  return init, apply\n","\n","\n","# Define modified MLP\n","def modified_MLP(layers, activation=relu):\n","  def xavier_init(key, d_in, d_out):\n","      glorot_stddev = 1. / np.sqrt((d_in + d_out) / 2.)\n","      W = glorot_stddev * random.normal(key, (d_in, d_out))\n","      b = np.zeros(d_out)\n","      return W, b\n","\n","  def init(rng_key):\n","      U1, b1 =  xavier_init(random.PRNGKey(12345), layers[0], layers[1])\n","      U2, b2 =  xavier_init(random.PRNGKey(54321), layers[0], layers[1])\n","      def init_layer(key, d_in, d_out):\n","          k1, k2 = random.split(key)\n","          W, b = xavier_init(k1, d_in, d_out)\n","          return W, b\n","      key, *keys = random.split(rng_key, len(layers))\n","      params = list(map(init_layer, keys, layers[:-1], layers[1:]))\n","      return (params, U1, b1, U2, b2)\n","\n","  def apply(params, inputs):\n","      params, U1, b1, U2, b2 = params\n","      U = activation(np.dot(inputs, U1) + b1)\n","      V = activation(np.dot(inputs, U2) + b2)\n","      for W, b in params[:-1]:\n","          outputs = activation(np.dot(inputs, W) + b)\n","          inputs = np.multiply(outputs, U) + np.multiply(1 - outputs, V)\n","      W, b = params[-1]\n","      outputs = np.dot(inputs, W) + b\n","      return outputs\n","  return init, apply\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Physics-Informed DeepONet"],"metadata":{"id":"3CIqLT2iiaAN"}},{"cell_type":"code","metadata":{"id":"kbh_Vdc8VmZZ"},"source":["# Define Physics-informed DeepONet model\n","class PI_DeepONet:\n","    def __init__(self, branch_layers, trunk_layers):\n","        # Network initialization and evaluation functions\n","        self.branch_init, self.branch_apply = modified_MLP(branch_layers, activation=np.tanh)\n","        self.trunk_init, self.trunk_apply = modified_MLP(trunk_layers, activation=np.tanh)\n","\n","        # Initialize\n","        branch_params = self.branch_init(rng_key = random.PRNGKey(1234))\n","        trunk_params = self.trunk_init(rng_key = random.PRNGKey(4321))\n","        params = (branch_params, trunk_params)\n","\n","        # Use optimizers to set optimizer initialization and update functions\n","        self.opt_init, \\\n","        self.opt_update, \\\n","        self.get_params = optimizers.adam(optimizers.exponential_decay(1e-3,\n","                                                                      decay_steps=2000,\n","                                                                      decay_rate=0.9))\n","        self.opt_state = self.opt_init(params)\n","\n","        # Used to restore the trained model parameters\n","        _, self.unravel_params = ravel_pytree(params)\n","\n","        # Logger\n","        self.itercount = itertools.count()\n","        self.loss_log = []\n","        self.loss_ics_log = []\n","        self.loss_bcs_log = []\n","        self.loss_res_log = []\n","\n","# Define DeepONet architecture\n","    def operator_net(self, params, u, t, x):\n","        branch_params, trunk_params = params\n","        y = np.stack([t,x])\n","        B = self.branch_apply(branch_params, u)\n","        T = self.trunk_apply(trunk_params, y)\n","        outputs = np.sum(B * T)\n","        return   outputs\n","\n","    # Define PDE residual\n","    def residual_net(self, params, u, t, x):\n","        s = self.operator_net(params, u, t, x)\n","        s_t = grad(self.operator_net, argnums=2)(params, u, t, x)\n","        s_x = grad(self.operator_net, argnums=3)(params, u, t, x)\n","        s_xx= grad(grad(self.operator_net, argnums=3), argnums=3)(params, u, t, x)\n","\n","        res = s_t + s * s_x - 0.01 * s_xx\n","        return res\n","\n","    # Define residual loss\n","    def loss_res(self, params, batch):\n","        # Fetch data\n","        inputs, outputs = batch\n","        u, y = inputs\n","        # Compute forward pass\n","        pred = vmap(self.residual_net, (None, 0, 0, 0))(params, u, y[:,0], y[:,1])\n","\n","        # Compute loss\n","        loss = np.mean((outputs.flatten() - pred)**2)\n","        return loss\n","\n","    # Define initial loss\n","    def loss_ics(self, params, batch):\n","        # Fetch data\n","        inputs, outputs = batch\n","        u, y = inputs\n","\n","        # Compute forward pass\n","        s_pred = vmap(self.operator_net, (None, 0, 0, 0))(params, u, y[:,0], y[:,1])\n","\n","        # Compute loss\n","        loss = np.mean((outputs.flatten() - s_pred)**2)\n","        return loss\n","\n","    # Define ds/dx\n","    def s_x_net(self, params, u, t, x):\n","         s_x = grad(self.operator_net, argnums=3)(params, u, t, x)\n","         return s_x\n","\n","    # Define boundary loss\n","    def loss_bcs(self, params, batch):\n","        # Fetch data\n","        inputs, outputs = batch\n","        u, y = inputs\n","\n","        # Compute forward pass\n","        s_bc1_pred = vmap(self.operator_net, (None, 0, 0, 0))(params, u, y[:,0], y[:,1])\n","        s_bc2_pred = vmap(self.operator_net, (None, 0, 0, 0))(params, u, y[:,2], y[:,3])\n","\n","        s_x_bc1_pred = vmap(self.s_x_net, (None, 0, 0, 0))(params, u, y[:,0], y[:,1])\n","        s_x_bc2_pred = vmap(self.s_x_net, (None, 0, 0, 0))(params, u, y[:,2], y[:,3])\n","\n","        # Compute loss\n","        loss_s_bc = np.mean((s_bc1_pred - s_bc2_pred)**2)\n","        loss_s_x_bc = np.mean((s_x_bc1_pred - s_x_bc2_pred)**2)\n","\n","        return loss_s_bc + loss_s_x_bc\n","\n","    # Define total loss\n","    def loss(self, params, ics_batch, bcs_batch, res_batch):\n","        loss_ics = self.loss_ics(params, ics_batch)\n","        loss_bcs = self.loss_bcs(params, bcs_batch)\n","        loss_res = self.loss_res(params, res_batch)\n","        loss =  20 * loss_ics + loss_bcs +  loss_res\n","        return loss\n","\n","    # Define a compiled update step\n","    @partial(jit, static_argnums=(0,))\n","    def step(self, i, opt_state, ics_batch, bcs_batch, res_batch):\n","        params = self.get_params(opt_state)\n","        g = grad(self.loss)(params, ics_batch, bcs_batch, res_batch)\n","        return self.opt_update(i, g, opt_state)\n","\n","    # Optimize parameters in a loop\n","    def train(self, ics_dataset, bcs_dataset, res_dataset, nIter = 10000):\n","        ics_data = iter(ics_dataset)\n","        bcs_data = iter(bcs_dataset)\n","        res_data = iter(res_dataset)\n","\n","        pbar = trange(nIter)\n","        # Main training loop\n","        for it in pbar:\n","            # Fetch data\n","            ics_batch= next(ics_data)\n","            bcs_batch= next(bcs_data)\n","            res_batch = next(res_data)\n","\n","            self.opt_state = self.step(next(self.itercount), self.opt_state, ics_batch, bcs_batch, res_batch)\n","\n","            if it % 100 == 0:\n","                params = self.get_params(self.opt_state)\n","\n","                # Compute losses\n","                loss_value = self.loss(params, ics_batch, bcs_batch, res_batch)\n","                loss_ics_value = self.loss_ics(params, ics_batch)\n","                loss_bcs_value = self.loss_bcs(params, bcs_batch)\n","                loss_res_value = self.loss_res(params, res_batch)\n","\n","                # Store losses\n","                self.loss_log.append(loss_value)\n","                self.loss_ics_log.append(loss_ics_value)\n","                self.loss_bcs_log.append(loss_bcs_value)\n","                self.loss_res_log.append(loss_res_value)\n","\n","                # Print losses\n","                pbar.set_postfix({'Loss': loss_value,\n","                                  'loss_ics' : loss_ics_value,\n","                                  'loss_bcs' : loss_bcs_value,\n","                                  'loss_physics': loss_res_value})\n","\n","# Evaluates predictions at test points\n","    @partial(jit, static_argnums=(0,))\n","    def predict_s(self, params, U_star, Y_star):\n","        s_pred = vmap(self.operator_net, (None, 0, 0, 0))(params, U_star, Y_star[:,0], Y_star[:,1])\n","        return s_pred\n","\n","    @partial(jit, static_argnums=(0,))\n","    def predict_res(self, params, U_star, Y_star):\n","        r_pred = vmap(self.residual_net, (None, 0, 0, 0))(params, U_star, Y_star[:,0], Y_star[:,1])\n","        return r_pred"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Generate Train Data"],"metadata":{"id":"1-060cvzekQo"}},{"cell_type":"code","metadata":{"id":"yPdIBu2xPoi4"},"source":["# Data generator\n","class DataGenerator(data.Dataset):\n","    def __init__(self, u, y, s,\n","                 batch_size=64, rng_key=random.PRNGKey(1234)):\n","        'Initialization'\n","        self.u = u\n","        self.y = y\n","        self.s = s\n","\n","        self.N = u.shape[0]\n","        self.batch_size = batch_size\n","        self.key = rng_key\n","\n","    def __getitem__(self, index):\n","        'Generate one batch of data'\n","        self.key, subkey = random.split(self.key)\n","        inputs, outputs = self.__data_generation(subkey)\n","        return inputs, outputs\n","\n","    @partial(jit, static_argnums=(0,))\n","    def __data_generation(self, key):\n","        'Generates data containing batch_size samples'\n","        idx = random.choice(key, self.N, (self.batch_size,), replace=False)\n","        s = self.s[idx,:]\n","        y = self.y[idx,:]\n","        u = self.u[idx,:]\n","        # Construct batch\n","        inputs = (u, y)\n","        outputs = s\n","        return inputs, outputs"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"k6NL6NI7P9Jo"},"source":["# Geneate ics training data corresponding to one input sample\n","def generate_one_ics_training_data(key, u0, m=101, P=101):\n","\n","    t_0 = np.zeros((P,1))\n","    x_0 = np.linspace(0, 1, P)[:, None]\n","\n","    y = np.hstack([t_0, x_0])\n","    u = np.tile(u0, (P, 1))\n","    s = u0\n","\n","    return u, y, s\n","\n","# Geneate bcs training data corresponding to one input sample\n","def generate_one_bcs_training_data(key, u0, m=101, P=100):\n","\n","    t_bc = random.uniform(key, (P,1))\n","    x_bc1 = np.zeros((P, 1))\n","    x_bc2 = np.ones((P, 1))\n","\n","    y1 = np.hstack([t_bc, x_bc1])  # shape = (P, 2)\n","    y2 = np.hstack([t_bc, x_bc2])  # shape = (P, 2)\n","\n","    u = np.tile(u0, (P, 1))\n","    y =  np.hstack([y1, y2])  # shape = (P, 4)\n","    s = np.zeros((P, 1))\n","\n","    return u, y, s\n","\n","# Geneate res training data corresponding to one input sample\n","def generate_one_res_training_data(key, u0, m=101, P=1000):\n","\n","    subkeys = random.split(key, 2)\n","\n","    t_res = random.uniform(subkeys[0], (P,1))\n","    x_res = random.uniform(subkeys[1], (P,1))\n","\n","    u = np.tile(u0, (P, 1))\n","    y =  np.hstack([t_res, x_res])\n","    s = np.zeros((P, 1))\n","\n","    return u, y, s\n","\n","# Geneate test data corresponding to one input sample\n","def generate_one_test_data(idx,usol, m=101, P=101):\n","\n","    u = usol[idx]\n","    u0 = u[0,:]\n","\n","    t = np.linspace(0, 1, m)\n","    x = np.linspace(0, 1, P)\n","    T, X = np.meshgrid(t, x)\n","\n","    s = u.T.flatten()\n","    u = np.tile(u0, (P**2, 1))\n","    y = np.hstack([T.flatten()[:,None], X.flatten()[:,None]])\n","\n","    return u, y, s\n","\n","# Geneate training data corresponding to N input sample\n","def compute_error(idx, usol, m, P):\n","    u_test, y_test, s_test = generate_one_test_data(idx, usol, m, P)\n","\n","    u_test = u_test.reshape(P**2,-1)\n","    y_test = y_test.reshape(P**2,-1)\n","    s_test = s_test.reshape(P**2,-1)\n","\n","    s_pred = model.predict_s(params, u_test, y_test)[:,None]\n","    error = np.linalg.norm(s_test - s_pred) / np.linalg.norm(s_test)\n","\n","    return error"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Specify Train Option\n"],"metadata":{"id":"fgezvJB1Drtl"}},{"cell_type":"code","metadata":{"id":"POD_ip6zwvWL"},"source":["# Prepare the training data\n","\n","# Load data\n","path = '/content/drive/MyDrive/Burger/Data/Burger.mat'  # Please use the matlab script to generate data\n","\n","data = scipy.io.loadmat(path)\n","usol = np.array( data['output'])\n","\n","N = usol.shape[0]  # number of total input samples\n","N_train =500    # number of input samples used for training (N)\n","N_test = N - N_train  # number of input samples used for test\n","m = 101           # number of sensors for input samples\n","P_ics_train = 101    # number of locations for evulating the initial condition\n","P_bcs_train = 100    # number of locations for evulating the boundary condition\n","P_res_train = 2500   # number of locations for evulating the PDE residual (Q)\n","P_test = 101        # resolution of uniform grid for the test data\n","u0_train = usol[:N_train,0,:]   # input samples\n","# usol_train = usol[:N_train,:,:]\n","\n","key = random.PRNGKey(0) # use different key for generating test data\n","keys = random.split(key, N_train)\n","\n","# Generate training data for inital condition\n","u_ics_train, y_ics_train, s_ics_train = vmap(generate_one_ics_training_data, in_axes=(0, 0, None, None))(keys, u0_train, m, P_ics_train)\n","\n","u_ics_train = u_ics_train.reshape(N_train * P_ics_train,-1)\n","y_ics_train = y_ics_train.reshape(N_train * P_ics_train,-1)\n","s_ics_train = s_ics_train.reshape(N_train * P_ics_train,-1)\n","\n","# Generate training data for boundary condition\n","u_bcs_train, y_bcs_train, s_bcs_train = vmap(generate_one_bcs_training_data, in_axes=(0, 0, None, None))(keys, u0_train, m, P_bcs_train)\n","\n","u_bcs_train = u_bcs_train.reshape(N_train * P_bcs_train,-1)\n","y_bcs_train = y_bcs_train.reshape(N_train * P_bcs_train,-1)\n","s_bcs_train = s_bcs_train.reshape(N_train * P_bcs_train,-1)\n","\n","# Generate training data for PDE residual\n","u_res_train, y_res_train, s_res_train = vmap(generate_one_res_training_data, in_axes=(0, 0, None, None))(keys, u0_train, m, P_res_train)\n","\n","u_res_train = u_res_train.reshape(N_train * P_res_train,-1)\n","y_res_train = y_res_train.reshape(N_train * P_res_train,-1)\n","s_res_train = s_res_train.reshape(N_train * P_res_train,-1)\n","\n","# Initialize model\n","branch_layers = [m, 5, 5, 5, 5, 5, 5, 5]\n","trunk_layers =  [2, 5, 5, 5, 5, 5, 5, 5]\n","\n","model = PI_DeepONet(branch_layers, trunk_layers)\n","\n","# Create data set\n","batch_size = 5000\n","ics_dataset = DataGenerator(u_ics_train, y_ics_train, s_ics_train, batch_size)\n","bcs_dataset = DataGenerator(u_bcs_train, y_bcs_train, s_bcs_train, batch_size)\n","res_dataset = DataGenerator(u_res_train, y_res_train, s_res_train, batch_size)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Train Network"],"metadata":{"id":"8Oov7jbze-V0"}},{"cell_type":"code","metadata":{"id":"gW8Ej5Jvhf0r"},"source":["# Train\n","# Note: may meet OOM issue if use Colab. Please train this model on the server.\n","model.train(ics_dataset, bcs_dataset, res_dataset, nIter=20000)\n","\n","# Restore the trained model\n","params = model.get_params(model.opt_state)\n","\n","#lam = 20\n","#params = model.unravel_params(np.load('TrainedModels/modified_MLP_lam_{}_params.npy'.format(lam)))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Evaluate Model Accurancy"],"metadata":{"id":"tErtt3qvfTH2"}},{"cell_type":"code","metadata":{"id":"05zrmpicNOHR"},"source":["# Compute relative l2 error over test data\n","idx = random.randint(key=random.PRNGKey(12345), shape=(400,), minval=N_train, maxval=2000)\n","k= 1500\n","N_test = 100\n","idx = np.arange(k, k + N_test)\n","\n","errors = vmap(compute_error, in_axes=(0, None, None, None))(idx, usol, m, P_test)\n","mean_error = errors.mean()\n","\n","print('Mean relative L2 error of s: {:.2e}'.format(mean_error))"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Plot for loss function\n","plt.figure(figsize = (6,5))\n","# plt.plot(model.loss_log, lw=2)\n","plt.plot(model.loss_ics_log, lw=2, label='ics')\n","plt.plot(model.loss_bcs_log, lw=2, label='bcs')\n","plt.plot(model.loss_res_log, lw=2, label='res')\n","\n","plt.xlabel('Iteration')\n","plt.ylabel('Loss')\n","plt.yscale('log')\n","plt.legend()\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"XrdiZk9nTD4-"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NicghBbsmLUH"},"source":["# Plot for one generated data\n","k = 103 # index\n","u = usol[k,:, :]\n","u0 = usol[k,0,:]\n","\n","P_test = 101\n","\n","t = np.linspace(0, 1, P_test)\n","x = np.linspace(0, 1, P_test)\n","T, X = np.meshgrid(t, x)\n","\n","u_test = np.tile(u0, (P_test**2, 1))\n","y_test = np.hstack([T.flatten()[:,None], X.flatten()[:,None]])\n","s_test = u.flatten()[:,None]\n","\n","s_pred = model.predict_s(params, u_test, y_test)[:,None]\n","S_pred = griddata(y_test, s_pred.flatten(), (T, X), method='cubic')\n","\n","\n","error_s = np.linalg.norm(u - S_pred.T, 2) / np.linalg.norm(u, 2)\n","\n","print(\"error_s: {:.3e}\".format(error_s))\n","\n","fig = plt.figure(figsize=(18,5))\n","plt.subplot(1,3,1)\n","plt.pcolor(T, X, u, cmap='jet')\n","plt.xlabel('$x$')\n","plt.ylabel('$t$')\n","plt.title('Exact $s(x,t)$')\n","plt.colorbar()\n","plt.tight_layout()\n","\n","plt.subplot(1,3,2)\n","plt.pcolor(T, X, S_pred.T, cmap='jet')\n","plt.xlabel('$x$')\n","plt.ylabel('$t$')\n","plt.title('Predict $s(x,t)$')\n","plt.colorbar()\n","plt.tight_layout()\n","\n","plt.subplot(1,3,3)\n","plt.pcolor(T, X, np.abs(S_pred.T - u), cmap='jet')\n","plt.xlabel('$x$')\n","plt.ylabel('$t$')\n","plt.title('Absolute error')\n","plt.colorbar()\n","plt.tight_layout()\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["본 페이지는 대한기계학회 고급인공지능 강습회의 가천대학교 김남중 교수님의 \"뉴럴 오퍼레이터를 통한 비지도 학습\"을 위해 작성된 페이지입니다. 해당 코드는 강연을 위해 약간의 수정이 있는 코드로 구글 코랩을 통해 돌아갈 수 있게 변경되었으니, github 코드를 통해 실행시킬 경우 코드 및 실행 방법이 약간 다를 수 있습니다. 본 페이지는 \"Physics-Informed Neural Operator for Learning Partial Differential Equations\"를 바탕으로 작성되었습니다.\n","\\\n","논문 링크 : https://doi.org/10.48550/arXiv.2103.10974\n","\\\n","Github 링크 : https://github.com/PredictiveIntelligenceLab/Physics-informed-DeepONets.git"],"metadata":{"id":"ihkJIpcgJ1WF"}}]}